{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model,model_selection,preprocessing\nimport os\nimport scipy\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-02T15:00:07.575535Z","iopub.execute_input":"2022-01-02T15:00:07.575846Z","iopub.status.idle":"2022-01-02T15:00:08.222561Z","shell.execute_reply.started":"2022-01-02T15:00:07.575764Z","shell.execute_reply":"2022-01-02T15:00:08.221876Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Funtions","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    h = 1/(1+np.exp(-x))\n    return h","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.224003Z","iopub.execute_input":"2022-01-02T15:00:08.225909Z","iopub.status.idle":"2022-01-02T15:00:08.231394Z","shell.execute_reply.started":"2022-01-02T15:00:08.225857Z","shell.execute_reply":"2022-01-02T15:00:08.230734Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def GradDesc(x,y,theta,alpha,reg_lambda = 0.001,num_iter = 100):\n    m = x.shape[0]\n    J_y = []\n    J_x = range(num_iter)\n    for i in range(num_iter):\n        z = np.dot(x,theta)\n        h = sigmoid(z)\n        J = -(np.dot(y.T,np.log(h)) + np.dot((1-y).T,np.log(1-h)))/m + reg_lambda*np.dot(theta.T,theta)/m\n        theta = theta - alpha * np.dot(x.T,(h-y))/m + alpha*reg_lambda*theta/m\n        J_y.append(float(J))\n    plt.plot(J_x,np.array(J_y))\n    plt.show()\n    J = float(J)\n    return J,theta","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.232651Z","iopub.execute_input":"2022-01-02T15:00:08.233501Z","iopub.status.idle":"2022-01-02T15:00:08.244134Z","shell.execute_reply.started":"2022-01-02T15:00:08.233455Z","shell.execute_reply":"2022-01-02T15:00:08.243439Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def PrepText(text):\n    \n    # remove old style retweet text \"RT\"\n    text = text.replace('\\n','')\n    text = str(text.encode('ascii','replace'))[2:]\n    text = re.sub(r'^RT[\\s]+', '', text)\n    # remove hyperlinks\n    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    text = re.sub(r'#', '', text)\n    # instantiate tokenizer class\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                                   reduce_len=True)\n    # tokenize tweets\n    tweet_tokens = tokenizer.tokenize(text)\n    stopwords_english = stopwords.words('english')\n    \n    clean_tweet = []\n\n    for word in tweet_tokens: # Go through every word in your tokens list\n        if (word not in stopwords_english and  # remove stopwords\n            word not in string.punctuation):  # remove punctuation\n            clean_tweet.append(word)\n            \n    # Instantiate stemming class\n    stemmer = PorterStemmer() \n\n    # Create an empty list to store the stems\n    tweet_fin = [] \n\n    for word in clean_tweet:\n        stem_word = stemmer.stem(word)  # stemming word\n        tweet_fin.append(stem_word)  # append to the list        \n            \n    return tweet_fin\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.246145Z","iopub.execute_input":"2022-01-02T15:00:08.246602Z","iopub.status.idle":"2022-01-02T15:00:08.257038Z","shell.execute_reply.started":"2022-01-02T15:00:08.246557Z","shell.execute_reply":"2022-01-02T15:00:08.256390Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def BuildFreqs(tweets, y):\n\n    ylist = np.squeeze(y).tolist()\n    freqs = {}\n    for y, tweet in zip(ylist, tweets):\n        for word in PrepText(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.258249Z","iopub.execute_input":"2022-01-02T15:00:08.258919Z","iopub.status.idle":"2022-01-02T15:00:08.275291Z","shell.execute_reply.started":"2022-01-02T15:00:08.258883Z","shell.execute_reply":"2022-01-02T15:00:08.274059Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def ExtFeats(tweet, freqs, PrepText=PrepText):\n\n    word_l = PrepText(tweet)\n    x = np.zeros((1, 3)) \n    x[0,0] = 1 # Default Bias\n\n    for word in word_l:\n        try:\n            x[0,1] += freqs[(word,1.0)]      \n        except:\n            continue\n        try:\n            x[0,2] += freqs[(word,0.0)]      \n        except:\n            continue\n        \n    return x","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.276943Z","iopub.execute_input":"2022-01-02T15:00:08.277181Z","iopub.status.idle":"2022-01-02T15:00:08.288952Z","shell.execute_reply.started":"2022-01-02T15:00:08.277153Z","shell.execute_reply":"2022-01-02T15:00:08.288084Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def PredictProba(tweet, freqs, theta):\n\n    x = ExtFeats(tweet,freqs)    \n    y_pred = sigmoid(np.dot(x,theta))\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.290518Z","iopub.execute_input":"2022-01-02T15:00:08.291021Z","iopub.status.idle":"2022-01-02T15:00:08.303329Z","shell.execute_reply.started":"2022-01-02T15:00:08.290987Z","shell.execute_reply":"2022-01-02T15:00:08.302366Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Data Development","metadata":{}},{"cell_type":"code","source":"# Importing Development Data\nDev = pd.read_csv(r'/kaggle/input/nlp-getting-started/train.csv',encoding='utf-8')\nDev.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.304725Z","iopub.execute_input":"2022-01-02T15:00:08.305065Z","iopub.status.idle":"2022-01-02T15:00:08.351847Z","shell.execute_reply.started":"2022-01-02T15:00:08.305036Z","shell.execute_reply":"2022-01-02T15:00:08.350966Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Visualising Positive and Negative Tweets:\ncookies = np.array([Dev.target.sum(),Dev.target.count() - Dev.target.sum()])\nplt.pie(cookies,labels = ['positive','negative'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.353110Z","iopub.execute_input":"2022-01-02T15:00:08.353480Z","iopub.status.idle":"2022-01-02T15:00:08.434464Z","shell.execute_reply.started":"2022-01-02T15:00:08.353444Z","shell.execute_reply":"2022-01-02T15:00:08.433772Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Train-Test Split:\nDev_X = Dev[['text']]\nDev_Y = Dev[['target']]\nTrain_X, Test_X, Train_y, Test_y = model_selection.train_test_split(Dev_X,Dev_Y,test_size = 0.3,random_state = 1)\nTrain_X = Train_X.reset_index(drop = True)\nTrain_X = list(Train_X.iloc[:,0])\nTest_X = Test_X.reset_index(drop = True)\nTest_X = list(Test_X.iloc[:,0])\nTrain_y = Train_y.reset_index(drop = True)\nTrain_y = Train_y.to_numpy()\nTest_y = Test_y.reset_index(drop = True)\nTest_y = Test_y.to_numpy()\nprint(\"Shape Of The Train Data: \",len(Train_X),\" Shape Of The Test Data: \",len(Test_X))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.435947Z","iopub.execute_input":"2022-01-02T15:00:08.436438Z","iopub.status.idle":"2022-01-02T15:00:08.454315Z","shell.execute_reply.started":"2022-01-02T15:00:08.436393Z","shell.execute_reply":"2022-01-02T15:00:08.453403Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Creating A Dictionary With Frequencies  ","metadata":{}},{"cell_type":"code","source":"freqs = BuildFreqs(Train_X,Train_y)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:08.455898Z","iopub.execute_input":"2022-01-02T15:00:08.456540Z","iopub.status.idle":"2022-01-02T15:00:12.243905Z","shell.execute_reply.started":"2022-01-02T15:00:08.456487Z","shell.execute_reply":"2022-01-02T15:00:12.242947Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Training The Model","metadata":{}},{"cell_type":"code","source":"X = np.zeros((len(Train_X), 3))\nfor i in range(len(Train_X)):\n    X[i, :]= ExtFeats(Train_X[i], freqs)\n    \nY = Train_y\n\nJ, theta = GradDesc(X, Y, theta = np.zeros((3, 1)),alpha = 1e-9,reg_lambda = 0,num_iter = 5000)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:12.245730Z","iopub.execute_input":"2022-01-02T15:00:12.246069Z","iopub.status.idle":"2022-01-02T15:00:18.708065Z","shell.execute_reply.started":"2022-01-02T15:00:12.246023Z","shell.execute_reply":"2022-01-02T15:00:18.707116Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"y_hat = []\n\nfor tweet in Test_X:\n    y_pred = PredictProba(tweet, freqs, theta)\n    \n    if y_pred > 0.5:\n        y_hat.append(1.0)\n    else:\n        y_hat.append(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:18.710830Z","iopub.execute_input":"2022-01-02T15:00:18.711183Z","iopub.status.idle":"2022-01-02T15:00:20.432865Z","shell.execute_reply.started":"2022-01-02T15:00:18.711148Z","shell.execute_reply":"2022-01-02T15:00:20.431868Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"a = np.array(y_hat) == Test_y.T\naccuracy = a[0].sum()/len(a[0])\nprint(\"Accuracy of The Model Is : \",np.round(accuracy*100,2), \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:20.434044Z","iopub.execute_input":"2022-01-02T15:00:20.434394Z","iopub.status.idle":"2022-01-02T15:00:20.441408Z","shell.execute_reply.started":"2022-01-02T15:00:20.434355Z","shell.execute_reply":"2022-01-02T15:00:20.440432Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Model For Submission","metadata":{}},{"cell_type":"code","source":"submission_data = pd.read_csv(r'/kaggle/input/nlp-getting-started/test.csv',encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:20.443051Z","iopub.execute_input":"2022-01-02T15:00:20.443472Z","iopub.status.idle":"2022-01-02T15:00:20.471458Z","shell.execute_reply.started":"2022-01-02T15:00:20.443426Z","shell.execute_reply":"2022-01-02T15:00:20.470604Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"submission_data_X = submission_data[['text']]\nsubmission_data_X = submission_data_X.reset_index(drop = True)\nsubmission_data_X = list(submission_data_X.iloc[:,0])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:20.472732Z","iopub.execute_input":"2022-01-02T15:00:20.473114Z","iopub.status.idle":"2022-01-02T15:00:20.479856Z","shell.execute_reply.started":"2022-01-02T15:00:20.473079Z","shell.execute_reply":"2022-01-02T15:00:20.479241Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"Dev_X = Dev[['text']]\nDev_Y = Dev[['target']]\n\nDev_X = Dev_X.reset_index(drop = True)\nDev_X = list(Dev_X.iloc[:,0])\n\nDev_Y = Dev_Y.reset_index(drop = True)\nDev_Y = Dev_Y.to_numpy()\n\nfreqs_depl = BuildFreqs(Dev_X,Dev_Y)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:20.481090Z","iopub.execute_input":"2022-01-02T15:00:20.481510Z","iopub.status.idle":"2022-01-02T15:00:25.711986Z","shell.execute_reply.started":"2022-01-02T15:00:20.481478Z","shell.execute_reply":"2022-01-02T15:00:25.711057Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"X_depl = np.zeros((len(Dev_X), 3))\nfor i in range(len(Dev_X)):\n    X_depl[i, :]= ExtFeats(Dev_X[i], freqs_depl)\n    \nY_depl = Dev_Y\n\nJ_depl, theta_depl = GradDesc(X_depl, Y_depl, theta = np.zeros((3, 1)),alpha = 1e-9,reg_lambda = 0,num_iter = 5000)\n\ny_hat_depl = []\n\nfor tweet in submission_data_X:\n    y_pred = PredictProba(tweet, freqs_depl, theta_depl)\n    \n    if y_pred > 0.5:\n        y_hat_depl.append(1.0)\n    else:\n        y_hat_depl.append(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:00:25.713382Z","iopub.execute_input":"2022-01-02T15:00:25.713632Z","iopub.status.idle":"2022-01-02T15:00:37.027794Z","shell.execute_reply.started":"2022-01-02T15:00:25.713600Z","shell.execute_reply":"2022-01-02T15:00:37.027015Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"sub_data = pd.concat([submission_data,pd.DataFrame(y_hat_depl,columns = ['target'])],axis = 1)\nsub_data.target = sub_data.target.astype('int64')\nsub_data[['id','target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:03:23.717167Z","iopub.execute_input":"2022-01-02T15:03:23.717660Z","iopub.status.idle":"2022-01-02T15:03:23.733976Z","shell.execute_reply.started":"2022-01-02T15:03:23.717624Z","shell.execute_reply":"2022-01-02T15:03:23.733043Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}