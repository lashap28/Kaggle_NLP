{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model,model_selection,preprocessing\nimport os\nimport scipy\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-04T12:36:39.506420Z","iopub.execute_input":"2022-01-04T12:36:39.506732Z","iopub.status.idle":"2022-01-04T12:36:41.272179Z","shell.execute_reply.started":"2022-01-04T12:36:39.506697Z","shell.execute_reply":"2022-01-04T12:36:41.271179Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def PrepText(text):\n    \n    # remove old style retweet text \"RT\"\n    text = text.replace('\\n','')\n    text = re.sub(r'^RT[\\s]+', '', text)\n    # remove hyperlinks\n    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    text = re.sub(r'#', '', text)\n    # instantiate tokenizer class\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                                   reduce_len=True)\n    # tokenize tweets\n    tweet_tokens = tokenizer.tokenize(text)\n    stopwords_english = stopwords.words('english')\n    \n    clean_tweet = []\n\n    for word in tweet_tokens: # Go through every word in your tokens list\n        if (word not in stopwords_english and  # remove stopwords\n            word not in string.punctuation):  # remove punctuation\n            clean_tweet.append(word)\n            \n    # Instantiate stemming class\n    stemmer = PorterStemmer() \n\n    # Create an empty list to store the stems\n    tweet_fin = [] \n\n    for word in clean_tweet:\n        stem_word = stemmer.stem(word)  # stemming word\n        tweet_fin.append(stem_word)  # append to the list        \n            \n    return tweet_fin\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:41.274181Z","iopub.execute_input":"2022-01-04T12:36:41.274439Z","iopub.status.idle":"2022-01-04T12:36:41.284115Z","shell.execute_reply.started":"2022-01-04T12:36:41.274407Z","shell.execute_reply":"2022-01-04T12:36:41.282790Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def BuildFreqs(tweets, y):\n\n    ylist = np.squeeze(y).tolist()\n    freqs = {}\n    for y, tweet in zip(ylist, tweets):\n        for word in PrepText(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:41.285318Z","iopub.execute_input":"2022-01-04T12:36:41.286074Z","iopub.status.idle":"2022-01-04T12:36:41.302164Z","shell.execute_reply.started":"2022-01-04T12:36:41.286033Z","shell.execute_reply":"2022-01-04T12:36:41.301388Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def ExtFeats(tweet, freqs, PrepText=PrepText):\n\n    word_l = PrepText(tweet)\n    x = np.zeros((1, 3)) \n    x[0,0] = 1 # Default Bias\n\n    for word in word_l:\n        try:\n            x[0,1] += freqs[(word,1.0)]      \n        except:\n            continue\n        try:\n            x[0,2] += freqs[(word,0.0)]      \n        except:\n            continue\n        \n    return x","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:41.304016Z","iopub.execute_input":"2022-01-04T12:36:41.304779Z","iopub.status.idle":"2022-01-04T12:36:41.316972Z","shell.execute_reply.started":"2022-01-04T12:36:41.304742Z","shell.execute_reply":"2022-01-04T12:36:41.315814Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def LookUp(freqs, word, label):\n    n = 0  # freqs.get((word, label), 0)\n\n    pair = (word, label)\n    if (pair in freqs):\n        n = freqs[pair]\n\n    return n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:41.318809Z","iopub.execute_input":"2022-01-04T12:36:41.319158Z","iopub.status.idle":"2022-01-04T12:36:41.330405Z","shell.execute_reply.started":"2022-01-04T12:36:41.319112Z","shell.execute_reply":"2022-01-04T12:36:41.329607Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def NaiveBayes(freqs,X,Y):\n    \n    loglikelihood = {}\n    logprior = 0\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)    # calculate N_pos and N_neg\n    N_pos = N_neg = 0\n    for pair in freqs.keys():\n        if pair[1] > 0:\n            N_pos += freqs[pair]\n        else:\n            N_neg += freqs[pair]\n\n    D = len(Y)\n    D_pos = Y.sum()\n    D_neg = D-D_pos\n    # Calculate logprior\n    logprior = np.log(D_pos/D_neg)\n    \n    for word in vocab:\n    # get the positive and negative frequency of the word\n        freq_pos = LookUp(freqs,word,1)\n        freq_neg = LookUp(freqs,word,0)\n\n        # calculate the probability that each word is positive, and negative\n        p_w_pos = (freq_pos + 1) / (N_pos + V)\n        p_w_neg = (freq_neg + 1) / (N_neg + V)\n\n        # calculate the log likelihood of the word\n        loglikelihood[word] = np.log(p_w_pos) - np.log(p_w_neg)\n\n    return logprior, loglikelihood\n  ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:41.333195Z","iopub.execute_input":"2022-01-04T12:36:41.333577Z","iopub.status.idle":"2022-01-04T12:36:41.347982Z","shell.execute_reply.started":"2022-01-04T12:36:41.333530Z","shell.execute_reply":"2022-01-04T12:36:41.347350Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def NaiveBayesPredict(tweet, logprior, loglikelihood):\n\n    word_l = PrepText(tweet)\n    p = 0\n    p += logprior\n\n    for word in word_l:\n        if word in loglikelihood:\n            p += loglikelihood[word]\n\n    return p","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:41.348889Z","iopub.execute_input":"2022-01-04T12:36:41.349426Z","iopub.status.idle":"2022-01-04T12:36:41.362440Z","shell.execute_reply.started":"2022-01-04T12:36:41.349395Z","shell.execute_reply":"2022-01-04T12:36:41.361677Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Importing And Splitting The Data","metadata":{}},{"cell_type":"code","source":"Dev_X = pd.read_csv('../input/nlp-getting-started/train.csv')[['text']]\nDev_Y = pd.read_csv('../input/nlp-getting-started/train.csv')[['target']]\nDev_X = Dev_X.reset_index(drop = True)\nDev_X_l = list(Dev_X.iloc[:,0])\nDev_Y = Dev_Y.reset_index(drop = True)\nDev_Y_l = Dev_Y.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:41:46.680088Z","iopub.execute_input":"2022-01-04T12:41:46.680574Z","iopub.status.idle":"2022-01-04T12:41:46.738609Z","shell.execute_reply.started":"2022-01-04T12:41:46.680534Z","shell.execute_reply":"2022-01-04T12:41:46.737771Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Visualising Positive and Negative Tweets:\na = int(Dev_Y.sum())\nb = int(Dev_Y.count() - Dev_Y.sum())\ncookies = np.array([a,b])\nplt.pie(cookies,labels = ['positive','negative'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:41.467391Z","iopub.execute_input":"2022-01-04T12:36:41.467662Z","iopub.status.idle":"2022-01-04T12:36:41.592315Z","shell.execute_reply.started":"2022-01-04T12:36:41.467634Z","shell.execute_reply":"2022-01-04T12:36:41.591308Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"Train_X, Test_X, Train_y,Test_y = model_selection.train_test_split(Dev_X,Dev_Y,test_size = 0.2, random_state = 1)\nTrain_X = Train_X.reset_index(drop = True)\nTrain_X = list(Train_X.iloc[:,0])\nTest_X = Test_X.reset_index(drop = True)\nTest_X = list(Test_X.iloc[:,0])\nTrain_y = Train_y.reset_index(drop = True)\nTrain_y = Train_y.to_numpy()\nTest_y = Test_y.reset_index(drop = True)\nTest_y = Test_y.to_numpy()\nprint(\"Shape Of The Train Data: \",len(Train_X),\" Shape Of The Test Data: \",len(Test_X))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:41.595046Z","iopub.execute_input":"2022-01-04T12:36:41.595779Z","iopub.status.idle":"2022-01-04T12:36:41.613756Z","shell.execute_reply.started":"2022-01-04T12:36:41.595732Z","shell.execute_reply":"2022-01-04T12:36:41.612685Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Generate Frequency","metadata":{}},{"cell_type":"code","source":"freqs = BuildFreqs(Train_X,Train_y)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:41.997805Z","iopub.execute_input":"2022-01-04T12:36:41.998703Z","iopub.status.idle":"2022-01-04T12:36:46.314956Z","shell.execute_reply.started":"2022-01-04T12:36:41.998662Z","shell.execute_reply":"2022-01-04T12:36:46.313729Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Training The Model","metadata":{}},{"cell_type":"code","source":"logprior, loglikelihood = NaiveBayes(freqs, Train_X, Train_y)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:46.316589Z","iopub.execute_input":"2022-01-04T12:36:46.316867Z","iopub.status.idle":"2022-01-04T12:36:46.387261Z","shell.execute_reply.started":"2022-01-04T12:36:46.316833Z","shell.execute_reply":"2022-01-04T12:36:46.386468Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"code","source":"y_hats = []\nfor tweet in Test_X:\n    # if the prediction is > 0\n    if NaiveBayesPredict(tweet, logprior, loglikelihood) > 0:\n        # the predicted class is 1\n        y_hat_i = 1\n    else:\n        # otherwise the predicted class is 0\n        y_hat_i = 0\n    # append the predicted class to the list y_hats\n    y_hats.append(y_hat_i)\nerror = np.mean(np.absolute(y_hats-Test_y))\n\n    # Accuracy is 1 minus the error\naccuracy = 1-error\nprint(\"Accuracy Is: \",accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:36:46.388254Z","iopub.execute_input":"2022-01-04T12:36:46.388677Z","iopub.status.idle":"2022-01-04T12:36:47.520745Z","shell.execute_reply.started":"2022-01-04T12:36:46.388640Z","shell.execute_reply":"2022-01-04T12:36:47.519687Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Prep. For Upload","metadata":{}},{"cell_type":"code","source":"deploy_X = pd.read_csv('../input/nlp-getting-started/test.csv')[['text']]\ndeploy_X = list(deploy_X.text)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:47:35.642000Z","iopub.execute_input":"2022-01-04T12:47:35.642384Z","iopub.status.idle":"2022-01-04T12:47:35.664895Z","shell.execute_reply.started":"2022-01-04T12:47:35.642343Z","shell.execute_reply":"2022-01-04T12:47:35.664031Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"freqs = BuildFreqs(Dev_X_l,Dev_Y_l)\n\nlogprior, loglikelihood = NaiveBayes(freqs, Dev_X_l, Dev_Y_l)\n\ny_hats = []\nfor tweet in Dev_X_l:\n    # if the prediction is > 0\n    if NaiveBayesPredict(tweet, logprior, loglikelihood) > 0:\n        # the predicted class is 1\n        y_hat_i = 1\n    else:\n        # otherwise the predicted class is 0\n        y_hat_i = 0\n    # append the predicted class to the list y_hats\n    y_hats.append(y_hat_i)\n\ny_hats_depl = []\nfor tweet in deploy_X:\n    # if the prediction is > 0\n    if NaiveBayesPredict(tweet, logprior, loglikelihood) > 0:\n        # the predicted class is 1\n        y_hat_i_depl = 1\n    else:\n        # otherwise the predicted class is 0\n        y_hat_i_depl = 0\n    # append the predicted class to the list y_hats\n    y_hats_depl.append(y_hat_i_depl)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:50:23.005240Z","iopub.execute_input":"2022-01-04T12:50:23.005543Z","iopub.status.idle":"2022-01-04T12:50:36.106849Z","shell.execute_reply.started":"2022-01-04T12:50:23.005510Z","shell.execute_reply":"2022-01-04T12:50:36.105870Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"res = pd.concat([pd.read_csv('../input/nlp-getting-started/test.csv')[['id']], pd.DataFrame(y_hats_depl,columns = ['target'])],axis = 1)\nres.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:53:51.442188Z","iopub.execute_input":"2022-01-04T12:53:51.442663Z","iopub.status.idle":"2022-01-04T12:53:51.476391Z","shell.execute_reply.started":"2022-01-04T12:53:51.442620Z","shell.execute_reply":"2022-01-04T12:53:51.475563Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}